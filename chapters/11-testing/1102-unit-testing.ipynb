{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f39d9d77-c336-42ad-a1a3-a4c28be72888",
   "metadata": {},
   "source": [
    "# Unit Testing\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c62714aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "current = Path.cwd()\n",
    "for parent in [current, *current.parents]:\n",
    "    if (parent / '_config.yml').exists():\n",
    "        project_root = parent  # ‚Üê Add project root, not chapters\n",
    "        break\n",
    "else:\n",
    "    project_root = Path.cwd().parent.parent\n",
    "\n",
    "sys.path.insert(0, str(project_root))\n",
    "\n",
    "from shared import thinkpython, diagram, jupyturtle\n",
    "from shared.download import download\n",
    "\n",
    "# Register as top-level modules so direct imports work in subsequent cells\n",
    "sys.modules['thinkpython'] = thinkpython\n",
    "sys.modules['diagram'] = diagram\n",
    "sys.modules['jupyturtle'] = jupyturtle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701137cb",
   "metadata": {},
   "source": [
    "## Unit Testing: Proactive Debugging\n",
    "\n",
    "Unit testing is a powerful debugging technique that prevents errors before they occur. Instead of waiting for bugs to appear and then hunting them down, you write tests that verify your code works correctly from the start.\n",
    "\n",
    "**Key Benefits of Unit Testing:**\n",
    "- **Catch bugs early**: Find problems before users do\n",
    "- **Regression prevention**: Ensure fixes don't break existing functionality  \n",
    "- **Documentation**: Tests show how your code should behave\n",
    "- **Confidence**: Make changes knowing tests will catch problems\n",
    "- **Systematic verification**: Methodically check all code paths\n",
    "\n",
    "**Basic Testing Concepts:**\n",
    "- **Test case**: A single test that checks one specific behavior\n",
    "- **Test suite**: A collection of related test cases\n",
    "- **Assertion**: A statement that checks if a condition is true\n",
    "- **Test runner**: Tool that executes tests and reports results\n",
    "\n",
    "Python's built-in `unittest` module provides everything you need to write and run tests."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a5885e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import unittest\n",
    "\n",
    "# Example: Testing a simple function with unit tests\n",
    "\n",
    "def calculate_grade(score, total_points):\n",
    "    \"\"\"Calculate percentage grade from score and total points.\"\"\"\n",
    "    if total_points <= 0:\n",
    "        raise ValueError(\"Total points must be positive\")\n",
    "    if score < 0:\n",
    "        raise ValueError(\"Score cannot be negative\") \n",
    "    if score > total_points:\n",
    "        raise ValueError(\"Score cannot exceed total points\")\n",
    "    \n",
    "    percentage = (score / total_points) * 100\n",
    "    return round(percentage, 2)\n",
    "\n",
    "def get_letter_grade(percentage):\n",
    "    \"\"\"Convert percentage to letter grade.\"\"\"\n",
    "    if percentage >= 90:\n",
    "        return 'A'\n",
    "    elif percentage >= 80:\n",
    "        return 'B'\n",
    "    elif percentage >= 70:\n",
    "        return 'C'\n",
    "    elif percentage >= 60:\n",
    "        return 'D'\n",
    "    else:\n",
    "        return 'F'\n",
    "\n",
    "# Unit tests for our functions\n",
    "class TestGradeCalculation(unittest.TestCase):\n",
    "    \"\"\"Test cases for grade calculation functions.\"\"\"\n",
    "    \n",
    "    def test_calculate_grade_normal_cases(self):\n",
    "        \"\"\"Test normal grade calculations.\"\"\"\n",
    "        self.assertEqual(calculate_grade(85, 100), 85.0)\n",
    "        self.assertEqual(calculate_grade(95, 100), 95.0)\n",
    "        self.assertEqual(calculate_grade(50, 100), 50.0)\n",
    "        self.assertEqual(calculate_grade(0, 100), 0.0)\n",
    "    \n",
    "    def test_calculate_grade_edge_cases(self):\n",
    "        \"\"\"Test edge cases for grade calculation.\"\"\"\n",
    "        self.assertEqual(calculate_grade(100, 100), 100.0)\n",
    "        self.assertEqual(calculate_grade(87, 92), 94.57)\n",
    "    \n",
    "    def test_calculate_grade_invalid_input(self):\n",
    "        \"\"\"Test that invalid inputs raise appropriate exceptions.\"\"\"\n",
    "        with self.assertRaises(ValueError):\n",
    "            calculate_grade(85, 0)  # Zero total points\n",
    "        \n",
    "        with self.assertRaises(ValueError):\n",
    "            calculate_grade(-10, 100)  # Negative score\n",
    "        \n",
    "        with self.assertRaises(ValueError):\n",
    "            calculate_grade(110, 100)  # Score exceeds total\n",
    "    \n",
    "    def test_letter_grade_conversion(self):\n",
    "        \"\"\"Test letter grade assignments.\"\"\"\n",
    "        self.assertEqual(get_letter_grade(95), 'A')\n",
    "        self.assertEqual(get_letter_grade(85), 'B')\n",
    "        self.assertEqual(get_letter_grade(75), 'C')\n",
    "        self.assertEqual(get_letter_grade(65), 'D')\n",
    "        self.assertEqual(get_letter_grade(55), 'F')\n",
    "        \n",
    "        # Test boundary conditions\n",
    "        self.assertEqual(get_letter_grade(90), 'A')\n",
    "        self.assertEqual(get_letter_grade(89.9), 'B')\n",
    "\n",
    "# Run the tests\n",
    "if __name__ == '__main__':\n",
    "    # Run tests and capture results\n",
    "    import io\n",
    "    import sys\n",
    "    \n",
    "    # Capture test output\n",
    "    test_output = io.StringIO()\n",
    "    runner = unittest.TextTestRunner(stream=test_output, verbosity=2)\n",
    "    suite = unittest.TestLoader().loadTestsFromTestCase(TestGradeCalculation)\n",
    "    result = runner.run(suite)\n",
    "    \n",
    "    # Display results\n",
    "    print(\"=== Unit Test Results ===\")\n",
    "    print(test_output.getvalue())\n",
    "    \n",
    "    if result.wasSuccessful():\n",
    "        print(\"‚úÖ All tests passed!\")\n",
    "    else:\n",
    "        print(f\"‚ùå {len(result.failures)} test(s) failed\")\n",
    "        print(f\"‚ùå {len(result.errors)} error(s) occurred\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f117eac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Test-Driven Debugging Workflow\n",
    "\n",
    "def find_maximum(numbers):\n",
    "    \"\"\"Find the maximum value in a list of numbers.\"\"\"\n",
    "    # Initial implementation with a bug\n",
    "    if not numbers:\n",
    "        return None\n",
    "    \n",
    "    max_val = numbers[0]\n",
    "    for num in numbers:\n",
    "        if num > max_val:\n",
    "            max_val = num\n",
    "    return max_val\n",
    "\n",
    "# Write tests first to define expected behavior\n",
    "class TestFindMaximum(unittest.TestCase):\n",
    "    \"\"\"Tests for find_maximum function.\"\"\"\n",
    "    \n",
    "    def test_normal_cases(self):\n",
    "        \"\"\"Test with normal lists of numbers.\"\"\"\n",
    "        self.assertEqual(find_maximum([1, 5, 3, 9, 2]), 9)\n",
    "        self.assertEqual(find_maximum([10, 20, 15]), 20)\n",
    "        self.assertEqual(find_maximum([7]), 7)\n",
    "    \n",
    "    def test_negative_numbers(self):\n",
    "        \"\"\"Test with negative numbers.\"\"\"\n",
    "        self.assertEqual(find_maximum([-1, -5, -3]), -1)\n",
    "        self.assertEqual(find_maximum([-10, 5, -3]), 5)\n",
    "    \n",
    "    def test_duplicates(self):\n",
    "        \"\"\"Test with duplicate maximum values.\"\"\"\n",
    "        self.assertEqual(find_maximum([5, 5, 5]), 5)\n",
    "        self.assertEqual(find_maximum([1, 3, 3, 2]), 3)\n",
    "    \n",
    "    def test_empty_list(self):\n",
    "        \"\"\"Test with empty list.\"\"\"\n",
    "        self.assertIsNone(find_maximum([]))\n",
    "    \n",
    "    def test_edge_cases(self):\n",
    "        \"\"\"Test edge cases.\"\"\"\n",
    "        self.assertEqual(find_maximum([0]), 0)\n",
    "        self.assertEqual(find_maximum([0, 0, 0]), 0)\n",
    "\n",
    "# Demonstrate the test-driven debugging process\n",
    "print(\"=== Test-Driven Debugging Example ===\")\n",
    "print(\"1. First, let's run our tests to see if our function works:\")\n",
    "\n",
    "# Run tests to find bugs\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestFindMaximum)\n",
    "test_output = io.StringIO()\n",
    "runner = unittest.TextTestRunner(stream=test_output, verbosity=2)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(test_output.getvalue())\n",
    "\n",
    "if result.wasSuccessful():\n",
    "    print(\"‚úÖ All tests pass! Our function is working correctly.\")\n",
    "else:\n",
    "    print(\"‚ùå Some tests failed. Let's debug using the test failures.\")\n",
    "    \n",
    "    # Show what manual testing would look like\n",
    "    print(\"\\n2. Manual debugging based on test failures:\")\n",
    "    test_cases = [\n",
    "        ([1, 5, 3, 9, 2], 9),\n",
    "        ([-1, -5, -3], -1),\n",
    "        ([]),\n",
    "    ]\n",
    "    \n",
    "    for numbers, expected in test_cases:\n",
    "        result = find_maximum(numbers)\n",
    "        status = \"‚úÖ\" if result == expected else \"‚ùå\"\n",
    "        print(f\"{status} find_maximum({numbers}) = {result}, expected: {expected}\")\n",
    "\n",
    "print(\"\\n3. The tests help us verify our function works correctly!\")\n",
    "print(\"   If there were bugs, the failing tests would show us exactly what to fix.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "947dc2e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Using Tests to Isolate and Fix Bugs\n",
    "\n",
    "class Calculator:\n",
    "    \"\"\"A simple calculator class with some bugs to find.\"\"\"\n",
    "    \n",
    "    def add(self, a, b):\n",
    "        return a + b\n",
    "    \n",
    "    def subtract(self, a, b):\n",
    "        return a - b\n",
    "    \n",
    "    def multiply(self, a, b):\n",
    "        return a * b\n",
    "    \n",
    "    def divide(self, a, b):\n",
    "        # Bug: Not handling division by zero\n",
    "        return a / b\n",
    "    \n",
    "    def power(self, base, exponent):\n",
    "        # Bug: Not handling negative bases correctly\n",
    "        if base < 0:\n",
    "            return \"Error: negative base\"\n",
    "        return base ** exponent\n",
    "\n",
    "# Comprehensive tests to find the bugs\n",
    "class TestCalculator(unittest.TestCase):\n",
    "    \"\"\"Tests to find bugs in Calculator class.\"\"\"\n",
    "    \n",
    "    def setUp(self):\n",
    "        \"\"\"Set up test calculator instance.\"\"\"\n",
    "        self.calc = Calculator()\n",
    "    \n",
    "    def test_addition(self):\n",
    "        \"\"\"Test addition operation.\"\"\"\n",
    "        self.assertEqual(self.calc.add(2, 3), 5)\n",
    "        self.assertEqual(self.calc.add(-1, 1), 0)\n",
    "        self.assertEqual(self.calc.add(0, 0), 0)\n",
    "    \n",
    "    def test_subtraction(self):\n",
    "        \"\"\"Test subtraction operation.\"\"\"\n",
    "        self.assertEqual(self.calc.subtract(5, 3), 2)\n",
    "        self.assertEqual(self.calc.subtract(1, 1), 0)\n",
    "        self.assertEqual(self.calc.subtract(-1, -1), 0)\n",
    "    \n",
    "    def test_multiplication(self):\n",
    "        \"\"\"Test multiplication operation.\"\"\"\n",
    "        self.assertEqual(self.calc.multiply(3, 4), 12)\n",
    "        self.assertEqual(self.calc.multiply(-2, 3), -6)\n",
    "        self.assertEqual(self.calc.multiply(0, 100), 0)\n",
    "    \n",
    "    def test_division_normal(self):\n",
    "        \"\"\"Test normal division cases.\"\"\"\n",
    "        self.assertEqual(self.calc.divide(10, 2), 5)\n",
    "        self.assertEqual(self.calc.divide(9, 3), 3)\n",
    "        self.assertAlmostEqual(self.calc.divide(1, 3), 0.333333, places=5)\n",
    "    \n",
    "    def test_division_by_zero(self):\n",
    "        \"\"\"Test division by zero - should raise exception.\"\"\"\n",
    "        with self.assertRaises(ZeroDivisionError):\n",
    "            self.calc.divide(10, 0)\n",
    "    \n",
    "    def test_power_positive(self):\n",
    "        \"\"\"Test power with positive bases.\"\"\"\n",
    "        self.assertEqual(self.calc.power(2, 3), 8)\n",
    "        self.assertEqual(self.calc.power(5, 0), 1)\n",
    "        self.assertEqual(self.calc.power(1, 100), 1)\n",
    "    \n",
    "    def test_power_negative_base(self):\n",
    "        \"\"\"Test power with negative bases.\"\"\"\n",
    "        # This test will fail due to our bug\n",
    "        self.assertEqual(self.calc.power(-2, 2), 4)\n",
    "        self.assertEqual(self.calc.power(-3, 3), -27)\n",
    "\n",
    "# Run tests to identify bugs\n",
    "print(\"=== Bug Detection with Unit Tests ===\")\n",
    "print(\"Running tests to identify bugs in Calculator class...\")\n",
    "\n",
    "suite = unittest.TestLoader().loadTestsFromTestCase(TestCalculator)\n",
    "test_output = io.StringIO()\n",
    "runner = unittest.TextTestRunner(stream=test_output, verbosity=2)\n",
    "result = runner.run(suite)\n",
    "\n",
    "print(test_output.getvalue())\n",
    "\n",
    "print(f\"\\nüìä Test Summary:\")\n",
    "print(f\"   Tests run: {result.testsRun}\")\n",
    "print(f\"   Failures: {len(result.failures)}\")\n",
    "print(f\"   Errors: {len(result.errors)}\")\n",
    "\n",
    "if result.failures:\n",
    "    print(f\"\\nüêõ Bugs found through testing:\")\n",
    "    for test, traceback in result.failures:\n",
    "        print(f\"   ‚Ä¢ {test}: Test revealed a bug in the implementation\")\n",
    "\n",
    "if result.errors:\n",
    "    print(f\"\\nüí• Errors found through testing:\")\n",
    "    for test, traceback in result.errors:\n",
    "        print(f\"   ‚Ä¢ {test}: Test revealed an error condition\")\n",
    "\n",
    "print(f\"\\n‚ú® This demonstrates how unit tests systematically find bugs!\")\n",
    "print(f\"   Without tests, these bugs might go unnoticed until production.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "019f5e82",
   "metadata": {},
   "source": [
    "### Unit Testing Best Practices for Debugging\n",
    "\n",
    "**1. Write Tests First (Test-Driven Development)**\n",
    "- Define expected behavior before implementing\n",
    "- Tests serve as specifications\n",
    "- Easier to catch bugs early\n",
    "\n",
    "**2. Test Edge Cases and Error Conditions**\n",
    "- Empty inputs, boundary values, invalid data\n",
    "- Exception handling scenarios\n",
    "- Unusual but valid inputs\n",
    "\n",
    "**3. Use Descriptive Test Names**\n",
    "- `test_divide_by_zero_raises_exception` vs `test_division`\n",
    "- Makes it clear what behavior is being tested\n",
    "- Easier to understand failures\n",
    "\n",
    "**4. Organize Tests Logically**\n",
    "- Group related tests in test classes\n",
    "- Use `setUp()` and `tearDown()` methods for common setup\n",
    "- Keep tests independent of each other\n",
    "\n",
    "**5. Assert Specific Behaviors**\n",
    "- Use appropriate assertion methods (`assertEqual`, `assertRaises`, etc.)\n",
    "- Test one behavior per test method\n",
    "- Make assertions clear and specific\n",
    "\n",
    "**Unit Testing as Debugging Strategy:**\n",
    "- **Prevention**: Catch bugs before they happen\n",
    "- **Isolation**: Quickly identify which component has the problem\n",
    "- **Regression**: Ensure fixes don't break existing functionality\n",
    "- **Documentation**: Tests show expected behavior\n",
    "- **Confidence**: Make changes knowing tests will catch issues"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd33555",
   "metadata": {},
   "source": [
    "# Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecdc88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uses_any(word, letters):\n",
    "    \"\"\"Checks if a word uses any of a list of letters.\n",
    "    \n",
    "    >>> uses_any('banana', 'aeiou')\n",
    "    True\n",
    "    >>> uses_any('apple', 'xyz')\n",
    "    False\n",
    "    \"\"\"\n",
    "    for letter in word.lower():\n",
    "        if letter in letters.lower():\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec62bdb0",
   "metadata": {},
   "source": [
    "Each test begins with `>>>`, which is used as a prompt in some Python environments to indicate where the user can type code.\n",
    "In a doctest, the prompt is followed by an expression, usually a function call.\n",
    "The following line indicates the value the expression should have if the function works correctly.\n",
    "\n",
    "In the first example, `'banana'` uses `'a'`, so the result should be `True`.\n",
    "In the second example, `'apple'` does not use any of `'xyz'`, so the result should be `False`.\n",
    "\n",
    "To run these tests, we have to import the `doctest` module and run a function called `run_docstring_examples`.\n",
    "To make this function easier to use, I wrote the following function, which takes a function object as an argument."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62bf2bad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from doctest import run_docstring_examples\n",
    "\n",
    "def run_doctests(func):\n",
    "    run_docstring_examples(func, globals(), name=func.__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2ca2fb8",
   "metadata": {},
   "source": [
    "We haven't learned about `globals` and `__name__` yet -- you can ignore them.\n",
    "Now we can test `uses_any` like this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c5c4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "run_doctests(uses_any)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df72a37e",
   "metadata": {},
   "source": [
    "`run_doctests` finds the expressions in the docstring and evaluates them.\n",
    "If the result is the expected value, the test **passes**.\n",
    "Otherwise it **fails**.\n",
    "\n",
    "If all tests pass, `run_doctests` displays no output -- in that case, no news is good news.\n",
    "To see what happens when a test fails, here's an incorrect version of `uses_any`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5974bb1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def uses_any_incorrect(word, letters):\n",
    "    \"\"\"Checks if a word uses any of a list of letters.\n",
    "    \n",
    "    >>> uses_any_incorrect('banana', 'aeiou')\n",
    "    True\n",
    "    >>> uses_any_incorrect('apple', 'xyz')\n",
    "    False\n",
    "    \"\"\"\n",
    "    for letter in word.lower():\n",
    "        if letter in letters.lower():\n",
    "            return True\n",
    "        else:\n",
    "            return False     # INCORRECT!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93b688f",
   "metadata": {},
   "source": [
    "And here's what happens when we test it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b81b70b5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**********************************************************************\n",
      "File \"__main__\", line 4, in uses_any_incorrect\n",
      "Failed example:\n",
      "    uses_any_incorrect('banana', 'aeiou')\n",
      "Expected:\n",
      "    True\n",
      "Got:\n",
      "    False\n"
     ]
    }
   ],
   "source": [
    "run_doctests(uses_any_incorrect)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d23ecfdd",
   "metadata": {},
   "source": [
    "The output includes the example that failed, the value the function was expected to produce, and the value the function actually produced.\n",
    "\n",
    "If you are not sure why this test failed, you'll have a chance to debug it as an exercise."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc1d3bd9",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
